# -*- coding: utf-8 -*-
"""SMTB P13 "48_K_codons_(FR)_+distant_important_seqs"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CnCX-8JAUER3cnkeYCaEPqSwuVNAxodm

### Скачаем данные
"""

!wget  https://www.dropbox.com/sh/snfhc92clh6t7ac/AAAn7SGvpJpZderYiKm14mgga --content-disposition &> /dev/null

"""### Разархивируем их"""

!unzip balanced_nonoverlapping_datasets.zip

import pandas as pd

df = pd.read_table("baseline.csv", sep=",")
df

"""### Скачаем вспомогательный код"""

!wget https://www.dropbox.com/s/uy9ugvsqxz92g7t/utils.py &> /dev/null

"""### Прочтем данные для выбранной аминокислоты"""

import pickle as pk
from utils import *

aa = "K" #put your value
context_len = 48 #put your value
train, test = pk.load(open(f"train_test_balanced_nonoverlapping_data_{aa}_{context_len}.pk", "rb"))
train_contexts, train_answers = train
test_contexts, test_answers = test

train_contexts[0], train_answers[0]

len(train_contexts)

"""### Ура! Теперь можно учить нейросетку"""

context_lenght = 48 #put your value

train_contexts_cut = cut_data(train_contexts, context_lenght, context_lenght)
test_contexts_cut = cut_data(test_contexts, context_lenght, context_lenght)

train_q, train_ans = recode_samples(train_contexts_cut, train_answers)
test_q, test_ans = recode_samples(test_contexts_cut, test_answers)

train_q[0].shape

"""#### А здесь можно попробовать сделать свою архитектуру сети"""

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPool1D, LSTM, Reshape, TimeDistributed
from tensorflow.keras.layers import  Concatenate, Softmax
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Activation, MultiHeadAttention, Dropout, Flatten, Dense, Input, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Lambda, Add, Attention
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop, SGD, Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
import numpy as np

N = len(set(train_answers))

inp = (Input((context_lenght*2+3, 4)))

cur = Conv1D(filters=64, kernel_size=6, activation='relu')(inp)
cur = Conv1D(filters=64, kernel_size=3, activation='relu')(cur)
cur = Conv1D(filters=64, kernel_size=3, activation='relu')(cur)
cur = Conv1D(filters=64, kernel_size=3, activation='relu')(cur)
cur = Conv1D(filters=64, kernel_size=3, activation='relu')(cur)

#cur = Bidirectional(LSTM(16, return_sequences=True, activation='relu'))(cur)

cur = MultiHeadAttention(num_heads=10, key_dim=4)(cur, cur)
cur = Dense(32, activation='relu')(cur)
cur = Dropout(0.1)(cur)
cur = Dense(16, activation='relu')(cur)
cur = Dropout(0.1)(cur)
#cur = GlobalAveragePooling1D()(cur)

cur = Flatten()(cur)
cur = Dense(16, activation='relu')(cur)
cur = Dropout(0.1)(cur)
cur = Dense(8, activation='relu')(cur)
cur = Dropout(0.1)(cur)

output = Dense(N, activation='softmax')(cur)
model = Model(inputs=inp, outputs=output)
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""### Следующие две клетки нужно раскомментировать, если вы хотите учить свою сетку. Раскомментировать большой кусок кода можно, выделив его и нажав Cntrl + /"""

# import datetime
# time = str(datetime.datetime.now())

# ep_now = 100
# batch_size = 128

# callbacks = [keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)]

# history = model.fit(train_q, train_ans, epochs=ep_now, batch_size=batch_size, validation_split=0.1, callbacks=callbacks)
# fname = 'model_{}_{}_{}_{}.h5'.format(aa, context_lenght, time, ep_now)
# model.save(fname)

# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('model accuracy')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'validation'], loc='upper left')
# plt.show()

# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['train', 'validation'], loc='upper left')
# plt.show()

# from google.colab import files

# files.download('model_{}_{}_{}_{}.h5'.format(aa, context_lenght, time, ep_now))

"""### Скачать и загрузить веса уже обученной модели"""

!wget https://www.dropbox.com/s/0n8ub6ka52d1glx/model_K_48_2022-07-29%2001_46_18.554808_100.h5

model = keras.models.load_model('model_K_48_2022-07-29 01_46_18.554808_100.h5')

"""#### Оценим качество получившейся нейросетки на независимом тестовом датасете"""

model.evaluate(test_q, test_ans)

"""### Матрица ошибок"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

#Predict
y_predicted = model.predict(test_q)

#get number of most probable codon for each seq
y_pred = np.argmax(y_predicted, axis=1)

#get number of true codon for each seq
y_true = np.argmax(test_ans, axis=1)

#Create confusion matrix
result = confusion_matrix(y_true, y_pred)

y_predicted[0], y_pred[0]

test_ans[0], y_true[0]

import matplotlib.pyplot as plt

codons = sorted(list(set(train_answers)))
disp = ConfusionMatrixDisplay(result, display_labels=codons)
disp.plot()
plt.show()

"""### Уверенность сетки в предсказании"""

from scipy.stats import entropy

confidence = [entropy(x, base=2) for x in y_predicted]

conf_0 = [confidence[i] for i in range(len(y_true)) if y_true[i] == 0 and (y_true[i] == y_pred[i])]
conf_1 = [confidence[i] for i in range(len(y_true)) if y_true[i] == 1 and (y_true[i] == y_pred[i])]

plt.hist([conf_0, conf_1], bins=20, density=True)
plt.legend(codons)
plt.show()

"""### Влияние точечных замен на предсказание"""

import seaborn as sns
from tqdm.notebook import tqdm

def get_permutational_feature_importance(x, model, state, n_states):
    predicted = np.zeros_like(x)
    hl = (len(x)-3)//2
    for ind in range(len(x)):
        if ind in set(range(hl, hl+3)):
            predicted[ind, :] = np.array([1/n_states]*4)
            continue
        mini_batch = []
        for i in range(4):
            xc = x.copy()
            xc[ind, :] = np.zeros([4], dtype=np.float32)
            xc[ind, i] = 1
            mini_batch.append(xc)
        mini_batch = np.array(mini_batch)
        predicted[ind,:] = model(mini_batch)[:,state]
    return predicted

def plot_mutational_FI(st_probs, codons, state, i, p):
    plt.figure(figsize=(20, 1))
    sns.heatmap(st_probs.T, yticklabels="ACGT", cmap="mako")
    plt.title(f"codon {codons[state]} probability heatmap for sequence point mutations in test seq #{i} (model_p={p})")
    plt.show()

state = 0

for i in tqdm(range(len(test_q))):
    state_probabilities = get_permutational_feature_importance(test_q[i], model, state, 2)
    if state_probabilities.max() > 0.5 and state_probabilities.min() < 0.5 and y_pred[i] == y_true[i]:
        p = model(np.array([test_q[i]]))[0,state]
        plot_mutational_FI(state_probabilities, codons, state, i, format(p,".3f"))

state_probabilities = get_permutational_feature_importance(test_q[10], model, state, 2)

np.array(model(np.array([test_q[10]])))

"""

1.   Для каждой последовательности из test_q и для каждой позиции в этой последовательности можно оценить "критичность" этой позиции - насколько ее изменение меняет предсказание.
2.   Какие влияют больше: ближние или дальние (поверх п.1.)
3. Какие мотивы больше влияют, находясь поблизости, а какие на отдалении? (поверх п.2.)
4. Среднее влияние (насколько изменилась вероятность кодона) при замене нуклеотида X на нуклеотид Y (в среднем по всем позиция).
5. Влияние по кодонным позициям (правда ли первые влияют больше, чем соответствующие вторые и третьи)?

"""

THRESHOLD = 0.7
influence_by_pos = [[] for _ in range(context_len*2+3)]

for i in tqdm(range(len(test_q))):
    state_probabilities = get_permutational_feature_importance(test_q[i], model, state, 2)
    if state_probabilities.max() > THRESHOLD and y_pred[i] == y_true[i]:
        for j in range(len(test_q[i])):
            influence_by_pos[j].append(max(state_probabilities[j])-min(state_probabilities[j]))

plt.figure(figsize=(10, 2))

mean_infs = [np.mean(x) for x in influence_by_pos]

plt.plot(mean_infs)
plt.xlabel("position")
plt.ylabel("mean influence")
plt.show()

def get_influetnce_by_pos(state_probabilities):
    res = []
    for i in range(len(state_probabilities)):
        res.append(max(state_probabilities[i])-min(state_probabilities[i]))
    return res

for i in tqdm(range(len(test_q))):
    state_probabilities = get_permutational_feature_importance(test_q[i], model, state, 2)
    if state_probabilities.max() > THRESHOLD and y_pred[i] == y_true[i]:
        infls = get_influetnce_by_pos(state_probabilities)
        max_pos = np.argmax(infls)
        if (max(infls[0:40]) > 0.5*max(infls) or max(infls[60:]) > 0.5*max(infls)) and max(infls) > 0.1:
            plt.figure(figsize=(10, 2))
            plt.plot(infls)
            plt.xlabel("position")
            plt.ylabel("max influence")
            plt.show()
            print(i)

cod_poss = [[] for _ in range(3)]

for i in list(range(42)) + list(range(60, 99)):
    cod_poss[i%3].append(mean_infs[i])

plt.boxplot(cod_poss)
plt.xlabel("codon position")
plt.ylabel("average influence")
plt.show()

np.argmax(test_q[0][5])

state_probabilities

"""### Функция, декодирующая последовательность обратно из вектора (может пригодиться)"""

def decode_seq(qu):
    lets = []
    bases_list = ["A", "C", "G", "T"]
    for q in qu:
        found = False
        for i in range(4):
            if q[i] == 1:
                lets.append(bases_list[i])
                found = True
        if not found:
            lets.append("_")
    assert len(lets) == len(qu)
    return lets

"".join(decode_seq(test_q[0]))

"""### Анализ сверточных фильтров - недоделано
TBD
"""

